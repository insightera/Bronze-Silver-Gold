# Metadata Management in Bronze to Silver ETL Pipeline - Part 2

**Patent Documentation: Lineage Tracking, Search & Discovery, Performance Optimization**

---

## ğŸ“‹ Table of Contents

1. [Spark Job Metadata Integration](#spark-job-metadata-integration)
2. [Silver Entity Registration](#silver-entity-registration)
3. [ETL Process & Lineage Creation](#etl-process--lineage-creation)
4. [Metadata Enrichment in Spark](#metadata-enrichment-in-spark)
5. [Search & Discovery with Solr](#search--discovery-with-solr)
6. [Lineage Querying & Visualization](#lineage-querying--visualization)
7. [Metadata Updates & Versioning](#metadata-updates--versioning)
8. [Performance Optimization](#performance-optimization)
9. [Scalability Considerations](#scalability-considerations)
10. [Patent Claims Summary](#patent-claims-summary)

---

## ğŸ”¥ Spark Job Metadata Integration

### Atlas Client in PySpark

**File**: `data-layer/spark-jobs/utils/atlas_client.py` (NEW)

```python
"""
Atlas Client for PySpark Jobs
Handles metadata registration and lineage tracking from Spark ETL jobs
"""

import requests
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


class AtlasClient:
    """
    Apache Atlas REST API client for PySpark jobs
    """
    
    def __init__(self, base_url: str, username: str, password: str):
        """
        Initialize Atlas client
        
        Args:
            base_url: Atlas server URL (e.g., http://70.153.84.76:21000)
            username: Atlas username
            password: Atlas password
        """
        self.base_url = base_url.rstrip('/')
        self.auth = (username, password)
        self.session = requests.Session()
        self.session.auth = self.auth
        self.session.headers.update({'Content-Type': 'application/json'})
        
        # Test connection
        self._test_connection()
    
    def _test_connection(self):
        """Verify Atlas connection"""
        try:
            response = self.session.get(f"{self.base_url}/api/atlas/admin/version")
            response.raise_for_status()
            version = response.json()
            logger.info(f"Connected to Atlas version: {version.get('Version', 'unknown')}")
        except Exception as e:
            logger.error(f"Failed to connect to Atlas: {e}")
            raise
    
    def get_entity_by_guid(self, guid: str) -> Dict[str, Any]:
        """
        Get entity by GUID
        
        Args:
            guid: Entity GUID
            
        Returns:
            Entity details
        """
        try:
            response = self.session.get(f"{self.base_url}/api/atlas/v2/entity/guid/{guid}")
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Failed to fetch entity {guid}: {e}")
            raise
    
    def create_entity(self, type_name: str, attributes: Dict[str, Any]) -> Dict[str, Any]:
        """
        Create new entity in Atlas
        
        Args:
            type_name: Entity type (e.g., 'adls_gen2_resource', 'etl_process')
            attributes: Entity attributes
            
        Returns:
            Created entity response with GUID assignments
        """
        entity = {
            "entity": {
                "typeName": type_name,
                "attributes": attributes,
                "status": "ACTIVE"
            }
        }
        
        try:
            logger.info(f"Creating Atlas entity: {type_name}")
            logger.debug(f"Attributes: {json.dumps(attributes, indent=2)}")
            
            response = self.session.post(
                f"{self.base_url}/api/atlas/v2/entity",
                json=entity
            )
            response.raise_for_status()
            
            result = response.json()
            guid = list(result.get('guidAssignments', {}).values())[0]
            logger.info(f"Entity created with GUID: {guid}")
            
            return result
        except Exception as e:
            logger.error(f"Failed to create entity {type_name}: {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"Response: {e.response.text}")
            raise
    
    def add_classifications(self, guid: str, classifications: List[str]):
        """
        Add classifications (tags) to entity
        
        Args:
            guid: Entity GUID
            classifications: List of classification names
        """
        try:
            payload = [{"typeName": name} for name in classifications]
            
            response = self.session.post(
                f"{self.base_url}/api/atlas/v2/entity/guid/{guid}/classifications",
                json=payload
            )
            response.raise_for_status()
            
            logger.info(f"Added classifications to {guid}: {classifications}")
        except Exception as e:
            logger.warning(f"Failed to add classifications: {e}")
            # Don't fail job if classification fails
    
    def create_adls_entity(self, 
                          qualified_name: str,
                          name: str,
                          container: str,
                          path: str,
                          file_type: str,
                          data_layer: str,
                          owner: str,
                          record_count: int = 0,
                          size_bytes: int = 0,
                          quality_score: float = 0.0,
                          additional_attrs: Optional[Dict] = None) -> str:
        """
        Create ADLS Gen2 resource entity
        
        Args:
            qualified_name: Unique identifier (e.g., "silver://akademik/...")
            name: Display name
            container: ADLS container (bronze/silver/gold)
            path: Path within container
            file_type: File type (csv/parquet/json)
            data_layer: Data layer (BRONZE/SILVER/GOLD)
            owner: Owner email
            record_count: Number of records
            size_bytes: File size in bytes
            quality_score: Data quality score (0-100)
            additional_attrs: Additional attributes
            
        Returns:
            Entity GUID
        """
        attributes = {
            "qualifiedName": qualified_name,
            "name": name,
            "storageAccount": "insighteradl",
            "container": container,
            "path": path,
            "fileType": file_type,
            "dataLayer": data_layer,
            "owner": owner,
            "recordCount": record_count,
            "sizeBytes": size_bytes,
            "qualityScore": quality_score,
            "processingDate": int(datetime.now().timestamp() * 1000)
        }
        
        # Merge additional attributes
        if additional_attrs:
            attributes.update(additional_attrs)
        
        result = self.create_entity("adls_gen2_resource", attributes)
        guid = list(result.get('guidAssignments', {}).values())[0]
        
        return guid
    
    def create_etl_process(self,
                          qualified_name: str,
                          name: str,
                          process_type: str,
                          input_guids: List[str],
                          output_guids: List[str],
                          transformations: Dict[str, Any],
                          execution_time: float,
                          executed_by: str,
                          additional_attrs: Optional[Dict] = None) -> str:
        """
        Create ETL process entity for lineage tracking
        
        Args:
            qualified_name: Unique process identifier
            name: Process display name
            process_type: Type (SPARK_ETL/AIRFLOW_DAG/etc)
            input_guids: List of input entity GUIDs
            output_guids: List of output entity GUIDs
            transformations: Transformation metadata
            execution_time: Execution time in seconds
            executed_by: User/system that executed
            additional_attrs: Additional attributes
            
        Returns:
            Process entity GUID
        """
        attributes = {
            "qualifiedName": qualified_name,
            "name": name,
            "processType": process_type,
            "inputs": [{"guid": guid} for guid in input_guids],
            "outputs": [{"guid": guid} for guid in output_guids],
            "transformations": json.dumps(transformations),
            "executionTime": execution_time,
            "executedBy": executed_by,
            "executionDate": int(datetime.now().timestamp() * 1000)
        }
        
        # Merge additional attributes
        if additional_attrs:
            attributes.update(additional_attrs)
        
        result = self.create_entity("etl_process", attributes)
        guid = list(result.get('guidAssignments', {}).values())[0]
        
        return guid
    
    def get_lineage(self, guid: str, direction: str = "BOTH", depth: int = 3) -> Dict[str, Any]:
        """
        Get entity lineage
        
        Args:
            guid: Entity GUID
            direction: Lineage direction (INPUT/OUTPUT/BOTH)
            depth: Lineage depth
            
        Returns:
            Lineage graph
        """
        try:
            params = {"direction": direction, "depth": depth}
            response = self.session.get(
                f"{self.base_url}/api/atlas/v2/lineage/{guid}",
                params=params
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Failed to fetch lineage for {guid}: {e}")
            raise


def create_atlas_client_from_config(config: Dict[str, Any]) -> AtlasClient:
    """
    Factory function to create Atlas client from Spark config
    
    Args:
        config: Spark job configuration containing Atlas credentials
        
    Returns:
        Configured AtlasClient instance
    """
    atlas_url = config.get('atlas_url', 'http://70.153.84.76:21000')
    atlas_username = config.get('atlas_username', 'admin')
    atlas_password = config.get('atlas_password', 'admin')
    
    return AtlasClient(atlas_url, atlas_username, atlas_password)
```

---

## ğŸ“Š Silver Entity Registration

### Enhanced Spark Job with Atlas Integration

**File**: `data-layer/spark-jobs/bronze_to_silver_transformation.py` (UPDATED)

```python
"""
Bronze to Silver Transformation with Atlas Metadata Registration
"""

import sys
import json
import logging
from datetime import datetime
from typing import Dict, Any

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F

# Import custom Atlas client
from utils.atlas_client import create_atlas_client_from_config, AtlasClient

logger = logging.getLogger(__name__)


class BronzeToSilverTransformer:
    """
    Enhanced transformer with Atlas metadata integration
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.upload_id = config['upload_id']
        self.bronze_atlas_guid = config.get('bronze_atlas_guid')
        self.bronze_path = config['bronze_path']
        self.silver_path_adls = config['silver_path_adls']
        self.data_mart_code = config['data_mart_code']
        
        # Initialize Spark
        self.spark = self._create_spark_session()
        
        # Initialize Atlas client
        self.atlas_client = create_atlas_client_from_config(config)
        
        # Metadata storage
        self.bronze_entity = None
        self.silver_guid = None
        self.etl_process_guid = None
        
        # Metrics tracking
        self.metrics = {
            'original_count': 0,
            'duplicates_removed': 0,
            'nulls_filled': 0,
            'type_conversions': 0,
            'final_count': 0,
            'quality_score': 0.0,
            'processing_time_seconds': 0.0,
            'columns_before': 0,
            'columns_after': 0
        }
    
    def _create_spark_session(self) -> SparkSession:
        """Create Spark session"""
        spark = SparkSession.builder \
            .appName(f"BronzeToSilver-{self.upload_id}") \
            .config("spark.sql.adaptive.enabled", "true") \
            .getOrCreate()
        
        return spark
    
    def fetch_bronze_metadata(self):
        """
        Fetch Bronze entity metadata from Atlas
        """
        if not self.bronze_atlas_guid:
            logger.warning("No Bronze GUID provided, skipping metadata fetch")
            return
        
        try:
            logger.info(f"Fetching Bronze metadata: {self.bronze_atlas_guid}")
            response = self.atlas_client.get_entity_by_guid(self.bronze_atlas_guid)
            self.bronze_entity = response.get('entity', {})
            
            logger.info(f"Bronze entity: {self.bronze_entity.get('attributes', {}).get('qualifiedName')}")
            logger.info(f"Bronze owner: {self.bronze_entity.get('attributes', {}).get('owner')}")
            
        except Exception as e:
            logger.error(f"Failed to fetch Bronze metadata: {e}")
            # Continue execution even if Atlas fetch fails
    
    def register_silver_entity(self, df: DataFrame) -> str:
        """
        Register Silver layer entity in Atlas
        
        Args:
            df: Final Silver DataFrame
            
        Returns:
            Silver entity GUID
        """
        try:
            # Build qualified name
            silver_qualified_name = (
                f"silver://{self.data_mart_code}/"
                f"{datetime.now().strftime('%Y/%m/%d')}/upload_{self.upload_id}"
                f"@insighteralake"
            )
            
            # Extract metadata from DataFrame
            record_count = df.count()
            columns_count = len(df.columns)
            
            # Inherit owner from Bronze entity
            owner = "etl_system"
            if self.bronze_entity:
                owner = self.bronze_entity.get('attributes', {}).get('owner', owner)
            
            # Calculate approximate size (for Parquet, estimate)
            size_estimate = record_count * columns_count * 50  # Rough estimate
            
            # Additional attributes
            additional_attrs = {
                "sourceUploadId": self.upload_id,
                "sourceBronzeGuid": self.bronze_atlas_guid,
                "transformationMetrics": json.dumps(self.metrics),
                "sparkApplicationId": self.spark.sparkContext.applicationId,
                "processingTimestamp": int(datetime.now().timestamp() * 1000)
            }
            
            # Create Silver entity
            logger.info("Registering Silver entity in Atlas...")
            silver_guid = self.atlas_client.create_adls_entity(
                qualified_name=silver_qualified_name,
                name=f"{self.data_mart_code}_upload_{self.upload_id}_silver",
                container="silver",
                path=self.silver_path_adls.split('silver/')[-1],
                file_type="parquet",
                data_layer="SILVER",
                owner=owner,
                record_count=record_count,
                size_bytes=size_estimate,
                quality_score=self.metrics.get('quality_score', 0.0),
                additional_attrs=additional_attrs
            )
            
            logger.info(f"âœ“ Silver entity registered: {silver_guid}")
            
            # Add Silver classification
            self.atlas_client.add_classifications(silver_guid, ['Silver'])
            
            # Copy PII/Sensitive classifications from Bronze
            if self.bronze_entity:
                bronze_classifications = self.bronze_entity.get('classifications', [])
                inherited_tags = [
                    cls.get('typeName') 
                    for cls in bronze_classifications 
                    if cls.get('typeName') in ['PII', 'Sensitive', 'Confidential']
                ]
                
                if inherited_tags:
                    logger.info(f"Inheriting classifications from Bronze: {inherited_tags}")
                    self.atlas_client.add_classifications(silver_guid, inherited_tags)
            
            self.silver_guid = silver_guid
            return silver_guid
            
        except Exception as e:
            logger.error(f"Failed to register Silver entity: {e}")
            # Don't fail job if Atlas registration fails
            return None
    
    def create_lineage(self) -> str:
        """
        Create ETL process entity for lineage tracking
        
        Returns:
            ETL process GUID
        """
        if not self.bronze_atlas_guid or not self.silver_guid:
            logger.warning("Missing Bronze or Silver GUID, skipping lineage creation")
            return None
        
        try:
            # Build qualified name for process
            process_qualified_name = f"bronze_to_silver_upload_{self.upload_id}"
            
            # Transformation details
            transformations = {
                "original_count": self.metrics['original_count'],
                "final_count": self.metrics['final_count'],
                "duplicates_removed": self.metrics['duplicates_removed'],
                "nulls_filled": self.metrics['nulls_filled'],
                "type_conversions": self.metrics['type_conversions'],
                "columns_before": self.metrics['columns_before'],
                "columns_after": self.metrics['columns_after'],
                "quality_score": self.metrics['quality_score'],
                "transformations_applied": [
                    "standardize_column_names",
                    "remove_duplicates",
                    "handle_missing_values",
                    "convert_data_types",
                    "add_metadata_columns",
                    "apply_quality_rules"
                ]
            }
            
            # Additional attributes
            additional_attrs = {
                "dagId": self.config.get('dag_id', 'bronze_to_silver_spark_pipeline'),
                "dagRunId": self.config.get('dag_run_id', ''),
                "sparkAppId": self.spark.sparkContext.applicationId,
                "uploadId": self.upload_id,
                "dataMartCode": self.data_mart_code
            }
            
            # Create ETL process
            logger.info("Creating ETL process entity for lineage...")
            etl_guid = self.atlas_client.create_etl_process(
                qualified_name=process_qualified_name,
                name=f"Bronzeâ†’Silver ETL: Upload {self.upload_id}",
                process_type="SPARK_ETL",
                input_guids=[self.bronze_atlas_guid],
                output_guids=[self.silver_guid],
                transformations=transformations,
                execution_time=self.metrics['processing_time_seconds'],
                executed_by="spark_etl_user",
                additional_attrs=additional_attrs
            )
            
            logger.info(f"âœ“ ETL process created: {etl_guid}")
            logger.info(f"âœ“ Lineage established: Bronze â†’ ETL â†’ Silver")
            
            self.etl_process_guid = etl_guid
            return etl_guid
            
        except Exception as e:
            logger.error(f"Failed to create lineage: {e}")
            return None
    
    def run(self) -> Dict[str, Any]:
        """
        Execute complete transformation with Atlas integration
        """
        start_time = datetime.now()
        
        try:
            # Step 1: Fetch Bronze metadata from Atlas
            self.fetch_bronze_metadata()
            
            # Step 2: Read Bronze data
            logger.info("Reading Bronze data...")
            df = self.read_bronze_data()
            self.metrics['original_count'] = df.count()
            self.metrics['columns_before'] = len(df.columns)
            
            # Step 3: Apply transformations
            logger.info("Applying transformations...")
            df = self.standardize_column_names(df)
            df = self.remove_duplicates(df)
            df = self.handle_missing_values(df)
            df = self.convert_data_types(df)
            df = self.add_metadata_columns(df)
            df, quality_metrics = self.apply_quality_rules(df)
            
            self.metrics['final_count'] = df.count()
            self.metrics['columns_after'] = len(df.columns)
            self.metrics['quality_score'] = quality_metrics.get('overall_score', 0.0)
            
            # Step 4: Write to Silver
            logger.info("Writing to Silver layer...")
            self.write_to_silver(df)
            
            # Step 5: Register Silver entity in Atlas
            logger.info("Registering metadata in Atlas...")
            silver_guid = self.register_silver_entity(df)
            
            # Step 6: Create lineage
            etl_guid = self.create_lineage()
            
            # Calculate execution time
            end_time = datetime.now()
            self.metrics['processing_time_seconds'] = (end_time - start_time).total_seconds()
            
            logger.info("=" * 60)
            logger.info("âœ“ TRANSFORMATION COMPLETED SUCCESSFULLY")
            logger.info(f"  Bronze GUID: {self.bronze_atlas_guid}")
            logger.info(f"  Silver GUID: {silver_guid}")
            logger.info(f"  ETL Process GUID: {etl_guid}")
            logger.info(f"  Records: {self.metrics['original_count']} â†’ {self.metrics['final_count']}")
            logger.info(f"  Quality Score: {self.metrics['quality_score']:.2f}%")
            logger.info(f"  Processing Time: {self.metrics['processing_time_seconds']:.2f}s")
            logger.info("=" * 60)
            
            return {
                'status': 'SUCCESS',
                'upload_id': self.upload_id,
                'bronze_atlas_guid': self.bronze_atlas_guid,
                'silver_atlas_guid': silver_guid,
                'etl_process_guid': etl_guid,
                'metrics': self.metrics,
                'quality_metrics': quality_metrics,
                'silver_path_adls': self.silver_path_adls
            }
            
        except Exception as e:
            logger.error(f"Transformation failed: {str(e)}", exc_info=True)
            
            return {
                'status': 'FAILED',
                'upload_id': self.upload_id,
                'error': str(e),
                'error_type': type(e).__name__,
                'metrics': self.metrics
            }
        
        finally:
            if self.spark:
                self.spark.stop()
    
    # ... (other transformation methods remain the same)
    # - read_bronze_data()
    # - standardize_column_names()
    # - remove_duplicates()
    # - handle_missing_values()
    # - convert_data_types()
    # - add_metadata_columns()
    # - apply_quality_rules()
    # - write_to_silver()


def main():
    """
    Main entry point
    """
    if len(sys.argv) != 2:
        print("Usage: bronze_to_silver_transformation.py <config_json>")
        sys.exit(1)
    
    # Parse configuration
    config = json.loads(sys.argv[1])
    
    # Run transformation
    transformer = BronzeToSilverTransformer(config)
    result = transformer.run()
    
    # Print result as JSON for Airflow to parse
    print(json.dumps(result))
    
    # Exit with appropriate code
    sys.exit(0 if result['status'] == 'SUCCESS' else 1)


if __name__ == '__main__':
    main()
```

---

## ğŸ”— ETL Process & Lineage Creation

### Lineage Graph Structure

```
Atlas Lineage Graph (Stored in HBase)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VERTEX: Bronze Entity                                           â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  GUID: a1b2c3d4-e5f6-7890-abcd-ef1234567890                     â”‚
â”‚  Type: adls_gen2_resource                                       â”‚
â”‚  QualifiedName: bronze://akademik/123/mahasiswa.csv             â”‚
â”‚                                                                  â”‚
â”‚  Properties:                                                     â”‚
â”‚  - name: mahasiswa.csv                                          â”‚
â”‚  - dataLayer: BRONZE                                            â”‚
â”‚  - recordCount: 1500                                            â”‚
â”‚  - owner: staff@univ.ac.id                                      â”‚
â”‚                                                                  â”‚
â”‚  Classifications: [Bronze]                                      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ EDGE: input
                           â”‚ (relationship: Process consumes Bronze)
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VERTEX: ETL Process Entity                                      â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  GUID: b2c3d4e5-f6a7-8901-bcde-f12345678901                     â”‚
â”‚  Type: etl_process                                              â”‚
â”‚  QualifiedName: bronze_to_silver_upload_123                     â”‚
â”‚                                                                  â”‚
â”‚  Properties:                                                     â”‚
â”‚  - name: Bronzeâ†’Silver ETL: Upload 123                          â”‚
â”‚  - processType: SPARK_ETL                                       â”‚
â”‚  - executionTime: 45.2                                          â”‚
â”‚  - executedBy: spark_etl_user                                   â”‚
â”‚  - executionDate: 1732780800000                                 â”‚
â”‚  - transformations: {                                            â”‚
â”‚      "duplicates_removed": 10,                                  â”‚
â”‚      "nulls_filled": 25,                                        â”‚
â”‚      "type_conversions": 5,                                     â”‚
â”‚      "quality_score": 98.3                                      â”‚
â”‚    }                                                             â”‚
â”‚  - dagId: bronze_to_silver_spark_pipeline                       â”‚
â”‚  - sparkAppId: app-20251128100000-0001                          â”‚
â”‚                                                                  â”‚
â”‚  Inputs: [a1b2c3d4-e5f6-7890-abcd-ef1234567890]                â”‚
â”‚  Outputs: [f9e8d7c6-b5a4-3210-fedc-ba0987654321]               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ EDGE: output
                           â”‚ (relationship: Process produces Silver)
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VERTEX: Silver Entity                                           â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  GUID: f9e8d7c6-b5a4-3210-fedc-ba0987654321                     â”‚
â”‚  Type: adls_gen2_resource                                       â”‚
â”‚  QualifiedName: silver://akademik/2025/11/28/upload_123         â”‚
â”‚                                                                  â”‚
â”‚  Properties:                                                     â”‚
â”‚  - name: akademik_upload_123_silver                             â”‚
â”‚  - dataLayer: SILVER                                            â”‚
â”‚  - recordCount: 1490                                            â”‚
â”‚  - owner: staff@univ.ac.id (inherited)                          â”‚
â”‚  - qualityScore: 98.3                                           â”‚
â”‚  - sourceBronzeGuid: a1b2c3d4-...                              â”‚
â”‚  - transformationMetrics: {...}                                 â”‚
â”‚                                                                  â”‚
â”‚  Classifications: [Silver, PII] (PII inherited from Bronze)     â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### HBase Storage Details

```
Table: atlas_entity
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Bronze Entity Row
Row Key: a1b2c3d4-e5f6-7890-abcd-ef1234567890
  cf_entity:
    typeName: "adls_gen2_resource"
    qualifiedName: "bronze://akademik/123/mahasiswa.csv"
    name: "mahasiswa.csv"
    dataLayer: "BRONZE"
    recordCount: 1500
    owner: "staff@univ.ac.id"
    container: "bronze"
    path: "akademik/123/mahasiswa.csv"
  cf_metadata:
    createdBy: "portal_backend"
    createTime: 1732780800000
    version: 1
  cf_classifications:
    Bronze: {"typeName": "Bronze"}

# ETL Process Entity Row
Row Key: b2c3d4e5-f6a7-8901-bcde-f12345678901
  cf_entity:
    typeName: "etl_process"
    qualifiedName: "bronze_to_silver_upload_123"
    name: "Bronzeâ†’Silver ETL: Upload 123"
    processType: "SPARK_ETL"
    executionTime: 45.2
    executedBy: "spark_etl_user"
    transformations: '{"duplicates_removed": 10, ...}'
    dagId: "bronze_to_silver_spark_pipeline"
    sparkAppId: "app-20251128100000-0001"
  cf_metadata:
    createdBy: "spark_etl"
    createTime: 1732780845000
    version: 1
  cf_relationships:
    inputs: ["a1b2c3d4-e5f6-7890-abcd-ef1234567890"]
    outputs: ["f9e8d7c6-b5a4-3210-fedc-ba0987654321"]

# Silver Entity Row
Row Key: f9e8d7c6-b5a4-3210-fedc-ba0987654321
  cf_entity:
    typeName: "adls_gen2_resource"
    qualifiedName: "silver://akademik/2025/11/28/upload_123"
    name: "akademik_upload_123_silver"
    dataLayer: "SILVER"
    recordCount: 1490
    qualityScore: 98.3
    sourceBronzeGuid: "a1b2c3d4-e5f6-7890-abcd-ef1234567890"
    transformationMetrics: '{"duplicates_removed": 10, ...}'
  cf_metadata:
    createdBy: "spark_etl"
    createTime: 1732780845000
    version: 1
  cf_classifications:
    Silver: {"typeName": "Silver"}
    PII: {"typeName": "PII", "inheritedFrom": "a1b2c3d4-..."}
```

---

## ğŸ” Search & Discovery with Solr

### Solr Index Schema

```
Collection: vertex_index
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Schema Fields:
â”â”â”â”â”â”â”â”â”â”â”â”â”
- guid (string, stored, indexed) - Entity GUID
- typeName (string, facet) - Entity type
- qualifiedName (string, stored, indexed) - Unique identifier
- name (text_general, stored) - Display name
- owner (string, facet, stored) - Owner email
- dataLayer (string, facet, stored) - BRONZE/SILVER/GOLD
- container (string, facet) - ADLS container
- recordCount (plong, stored) - Number of records
- qualityScore (pdouble, stored) - Quality score
- createTime (pdate, stored, indexed) - Creation timestamp
- modifyTime (pdate, stored) - Modification timestamp
- classifications (strings, multivalued, facet) - Tags
- text (text_general) - Full-text searchable content
- dataLayer_s (string, facet) - Facet copy of dataLayer
- typeName_s (string, facet) - Facet copy of typeName
- owner_s (string, facet) - Facet copy of owner
```

### Search Query Examples

#### 1. Find all Silver layer entities

```bash
# Solr Query
curl "http://localhost:8983/solr/vertex_index/select" \
  -d "q=dataLayer_s:SILVER" \
  -d "rows=100" \
  -d "sort=createTime desc"

# Result
{
  "response": {
    "numFound": 245,
    "docs": [
      {
        "guid": "f9e8d7c6-b5a4-3210-fedc-ba0987654321",
        "typeName": "adls_gen2_resource",
        "name": "akademik_upload_123_silver",
        "dataLayer": "SILVER",
        "owner": "staff@univ.ac.id",
        "recordCount": 1490,
        "qualityScore": 98.3,
        "createTime": "2025-11-28T10:01:30Z"
      },
      // ... more results
    ]
  }
}
```

#### 2. Faceted search - Count by data layer

```bash
# Solr Query
curl "http://localhost:8983/solr/vertex_index/select" \
  -d "q=*:*" \
  -d "rows=0" \
  -d "facet=true" \
  -d "facet.field=dataLayer_s" \
  -d "facet.field=typeName_s" \
  -d "facet.field=owner_s"

# Result
{
  "facet_counts": {
    "facet_fields": {
      "dataLayer_s": [
        "BRONZE", 1250,
        "SILVER", 245,
        "GOLD", 18
      ],
      "typeName_s": [
        "adls_gen2_resource", 1480,
        "etl_process", 245,
        "publication_dataset", 33
      ],
      "owner_s": [
        "staff@univ.ac.id", 850,
        "pimpinan@univ.ac.id", 320,
        "dosen@univ.ac.id", 543
      ]
    }
  }
}
```

#### 3. Full-text search for "mahasiswa"

```bash
# Solr Query
curl "http://localhost:8983/solr/vertex_index/select" \
  -d "q=text:mahasiswa" \
  -d "rows=20" \
  -d "fl=guid,name,qualifiedName,dataLayer" \
  -d "hl=true" \
  -d "hl.fl=text"

# Result - Finds entities with "mahasiswa" in name, path, or description
{
  "response": {
    "numFound": 15,
    "docs": [
      {
        "guid": "a1b2c3d4-...",
        "name": "mahasiswa.csv",
        "qualifiedName": "bronze://akademik/123/mahasiswa.csv",
        "dataLayer": "BRONZE"
      },
      {
        "guid": "f9e8d7c6-...",
        "name": "akademik_upload_123_silver",
        "qualifiedName": "silver://akademik/2025/11/28/upload_123",
        "dataLayer": "SILVER"
      }
    ]
  },
  "highlighting": {
    "a1b2c3d4-...": {
      "text": ["...bronze akademik <em>mahasiswa</em>.csv..."]
    }
  }
}
```

#### 4. Filter by quality score range

```bash
# Solr Query - Find high-quality Silver entities (score >= 95)
curl "http://localhost:8983/solr/vertex_index/select" \
  -d "q=dataLayer_s:SILVER AND qualityScore:[95 TO *]" \
  -d "rows=50" \
  -d "sort=qualityScore desc"

# Result
{
  "response": {
    "numFound": 182,
    "docs": [
      {
        "guid": "f9e8d7c6-...",
        "name": "akademik_upload_123_silver",
        "qualityScore": 98.3,
        "recordCount": 1490
      },
      // ... more high-quality results
    ]
  }
}
```

#### 5. Date range search - Last 7 days

```bash
# Solr Query
curl "http://localhost:8983/solr/vertex_index/select" \
  -d "q=createTime:[NOW-7DAYS TO NOW]" \
  -d "rows=100" \
  -d "sort=createTime desc"
```

#### 6. Classification-based search - Find PII data

```bash
# Solr Query
curl "http://localhost:8983/solr/vertex_index/select" \
  -d "q=classifications:PII" \
  -d "fq=dataLayer_s:SILVER" \
  -d "rows=50"

# Result - All Silver entities tagged with PII
{
  "response": {
    "numFound": 23,
    "docs": [
      {
        "guid": "f9e8d7c6-...",
        "name": "akademik_upload_123_silver",
        "classifications": ["Silver", "PII"],
        "owner": "staff@univ.ac.id"
      }
    ]
  }
}
```

### Backend Search API Integration

**File**: `backend/src/controllers/atlas.controller.js`

```javascript
/**
 * Search entities with Solr-powered search
 */
async function searchEntities(req, res) {
  try {
    const {
      query = '*',
      type = null,
      classification = null,
      dataLayer = null,
      owner = null,
      dateFrom = null,
      dateTo = null,
      minQualityScore = null,
      limit = 50,
      offset = 0
    } = req.query;
    
    // Build Solr filter queries
    const filters = [];
    
    if (type) filters.push(`typeName_s:${type}`);
    if (classification) filters.push(`classifications:${classification}`);
    if (dataLayer) filters.push(`dataLayer_s:${dataLayer}`);
    if (owner) filters.push(`owner_s:"${owner}"`);
    if (minQualityScore) filters.push(`qualityScore:[${minQualityScore} TO *]`);
    
    // Date range filter
    if (dateFrom || dateTo) {
      const from = dateFrom || '*';
      const to = dateTo || '*';
      filters.push(`createTime:[${from} TO ${to}]`);
    }
    
    // Call Atlas search API with filters
    const searchParams = {
      query,
      limit,
      offset
    };
    
    // Add type filter if specified
    if (type) searchParams.typeName = type;
    if (classification) searchParams.classification = classification;
    
    const results = await atlasService.searchEntitiesBasic(
      query,
      type,
      classification,
      limit,
      offset,
      true // enrichWithDetails
    );
    
    // Apply additional filters in memory (for quality score, etc.)
    let filteredEntities = results.entities || [];
    
    if (dataLayer) {
      filteredEntities = filteredEntities.filter(
        e => e.attributes?.dataLayer === dataLayer
      );
    }
    
    if (owner) {
      filteredEntities = filteredEntities.filter(
        e => e.attributes?.owner === owner
      );
    }
    
    if (minQualityScore) {
      filteredEntities = filteredEntities.filter(
        e => (e.attributes?.qualityScore || 0) >= parseFloat(minQualityScore)
      );
    }
    
    res.json({
      success: true,
      total: filteredEntities.length,
      entities: filteredEntities,
      facets: results.facets || {}
    });
    
  } catch (error) {
    console.error('[Atlas Search] Error:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
}
```

---

## ğŸ“ˆ Lineage Querying & Visualization

### Lineage Query API

**File**: `backend/src/controllers/atlas.controller.js`

```javascript
/**
 * Get entity lineage
 */
async function getEntityLineage(req, res) {
  try {
    const { guid } = req.params;
    const { direction = 'BOTH', depth = 3 } = req.query;
    
    // Fetch lineage from Atlas
    const lineage = await atlasService.getEntityLineage(guid, direction, parseInt(depth));
    
    // Transform lineage for frontend visualization
    const graph = transformLineageToGraph(lineage);
    
    res.json({
      success: true,
      guid,
      direction,
      depth,
      lineage: graph
    });
    
  } catch (error) {
    console.error('[Atlas Lineage] Error:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
}

/**
 * Transform Atlas lineage format to graph structure
 */
function transformLineageToGraph(atlasLineage) {
  const nodes = [];
  const edges = [];
  const processedGuids = new Set();
  
  // Extract base entity
  const baseEntity = atlasLineage.baseEntityGuid;
  
  // Process relations
  if (atlasLineage.relations) {
    atlasLineage.relations.forEach(relation => {
      const fromGuid = relation.fromEntityId;
      const toGuid = relation.toEntityId;
      
      // Add nodes
      if (!processedGuids.has(fromGuid)) {
        nodes.push(extractNodeFromLineage(atlasLineage, fromGuid));
        processedGuids.add(fromGuid);
      }
      
      if (!processedGuids.has(toGuid)) {
        nodes.push(extractNodeFromLineage(atlasLineage, toGuid));
        processedGuids.add(toGuid);
      }
      
      // Add edge
      edges.push({
        from: fromGuid,
        to: toGuid,
        label: relation.relationshipType || 'dataflow',
        processId: relation.processId
      });
    });
  }
  
  return {
    nodes,
    edges,
    baseEntityGuid: baseEntity
  };
}

function extractNodeFromLineage(lineage, guid) {
  // Find entity in lineage.guidEntityMap
  const entity = lineage.guidEntityMap?.[guid];
  
  if (!entity) {
    return {
      id: guid,
      label: 'Unknown',
      type: 'unknown'
    };
  }
  
  return {
    id: guid,
    label: entity.displayText || entity.attributes?.name || guid.substring(0, 8),
    type: entity.typeName,
    attributes: entity.attributes,
    classifications: entity.classifications || []
  };
}
```

### Lineage Visualization Example

```javascript
// Frontend visualization data structure
const lineageGraph = {
  "nodes": [
    {
      "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
      "label": "mahasiswa.csv",
      "type": "adls_gen2_resource",
      "attributes": {
        "dataLayer": "BRONZE",
        "recordCount": 1500
      },
      "classifications": ["Bronze"],
      "color": "#3498db" // Blue for Bronze
    },
    {
      "id": "b2c3d4e5-f6a7-8901-bcde-f12345678901",
      "label": "Bronzeâ†’Silver ETL",
      "type": "etl_process",
      "attributes": {
        "processType": "SPARK_ETL",
        "executionTime": 45.2
      },
      "color": "#9b59b6" // Purple for Process
    },
    {
      "id": "f9e8d7c6-b5a4-3210-fedc-ba0987654321",
      "label": "akademik_upload_123_silver",
      "type": "adls_gen2_resource",
      "attributes": {
        "dataLayer": "SILVER",
        "recordCount": 1490,
        "qualityScore": 98.3
      },
      "classifications": ["Silver", "PII"],
      "color": "#95a5a6" // Silver for Silver
    }
  ],
  "edges": [
    {
      "from": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
      "to": "b2c3d4e5-f6a7-8901-bcde-f12345678901",
      "label": "input",
      "arrow": "to"
    },
    {
      "from": "b2c3d4e5-f6a7-8901-bcde-f12345678901",
      "to": "f9e8d7c6-b5a4-3210-fedc-ba0987654321",
      "label": "output",
      "arrow": "to"
    }
  ],
  "baseEntityGuid": "f9e8d7c6-b5a4-3210-fedc-ba0987654321"
};
```

---

## ğŸ”„ Metadata Updates & Versioning

### Update Bronze Entity After Staging

```javascript
// backend/src/services/staging.service.js

async function updateBronzeMetadataAfterStaging(uploadId, stagingResults) {
  const upload = await prisma.upload.findUnique({
    where: { id: uploadId }
  });
  
  if (!upload.atlasGuid) {
    console.warn('[Staging] No Atlas GUID, skipping metadata update');
    return;
  }
  
  try {
    // Update Bronze entity with staging results
    const updatedAttributes = {
      stagingScore: stagingResults.overallScore,
      stagingCompletedAt: new Date().getTime(),
      stagingMetrics: JSON.stringify({
        completenessScore: stagingResults.completenessScore,
        uniquenessScore: stagingResults.uniquenessScore,
        validityScore: stagingResults.validityScore,
        consistencyScore: stagingResults.consistencyScore
      })
    };
    
    await atlasService.updateEntity(upload.atlasGuid, updatedAttributes);
    
    console.log(`[Staging] Updated Bronze metadata: ${upload.atlasGuid}`);
    
  } catch (error) {
    console.error('[Staging] Failed to update Atlas metadata:', error);
    // Don't fail staging if Atlas update fails
  }
}
```

### Atlas Entity Versioning

```
HBase Row (Before Update):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Row: a1b2c3d4-e5f6-7890-abcd-ef1234567890
  cf_entity:
    recordCount: 1500
  cf_metadata:
    version: 1
    modifyTime: 1732780800000

HBase Row (After Update):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Row: a1b2c3d4-e5f6-7890-abcd-ef1234567890
  cf_entity:
    recordCount: 1500
    stagingScore: 95.5
    stagingCompletedAt: 1732780900000
    stagingMetrics: '{"completenessScore": 98.5, ...}'
  cf_metadata:
    version: 2  â† Incremented
    modifyTime: 1732780900000  â† Updated
    modifiedBy: "staging_service"

Audit Log (separate table):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Table: atlas_audit
Row: a1b2c3d4-..._v2_1732780900000
  cf_audit:
    entityGuid: "a1b2c3d4-..."
    version: 2
    action: "ENTITY_UPDATE"
    timestamp: 1732780900000
    user: "staging_service"
    changes: '{"stagingScore": nullâ†’95.5, "stagingCompletedAt": nullâ†’1732780900000}'
```

---

## âš¡ Performance Optimization

### 1. Bulk Entity Creation

For batch uploads (multiple files), create entities in bulk:

```python
# atlas_client.py - Bulk operations

def create_entities_bulk(self, entities: List[Dict[str, Any]]) -> Dict[str, str]:
    """
    Create multiple entities in single API call
    
    Args:
        entities: List of entity definitions
        
    Returns:
        Dict mapping temp IDs to GUIDs
    """
    payload = {
        "entities": [
            {
                "typeName": entity["typeName"],
                "attributes": entity["attributes"],
                "status": "ACTIVE"
            }
            for entity in entities
        ]
    }
    
    response = self.session.post(
        f"{self.base_url}/api/atlas/v2/entity/bulk",
        json=payload
    )
    response.raise_for_status()
    
    return response.json().get('guidAssignments', {})
```

### 2. Caching Atlas Metadata

Cache frequently accessed metadata in Redis:

```javascript
// backend/src/services/atlas-cache.service.js

const redis = require('redis');
const client = redis.createClient();

async function getCachedEntity(guid) {
  const cacheKey = `atlas:entity:${guid}`;
  
  // Try cache first
  const cached = await client.get(cacheKey);
  if (cached) {
    return JSON.parse(cached);
  }
  
  // Fetch from Atlas
  const entity = await atlasService.getEntityByGuid(guid);
  
  // Cache for 5 minutes
  await client.setEx(cacheKey, 300, JSON.stringify(entity));
  
  return entity;
}
```

### 3. Async Atlas Registration

Don't block upload on Atlas registration:

```javascript
// backend/src/controllers/file.controller.js

async function uploadFileToBronze(req, res) {
  // ... upload to ADLS and create DB record ...
  
  // Return response immediately
  res.json({
    success: true,
    uploadId: upload.id
  });
  
  // Register in Atlas asynchronously (don't await)
  registerInAtlasAsync(upload).catch(error => {
    console.error('[Atlas] Background registration failed:', error);
  });
}

async function registerInAtlasAsync(upload) {
  // Perform Atlas registration in background
  const atlasResponse = await atlasService.createADLSEntity({...});
  
  // Update database with GUID
  await prisma.upload.update({
    where: { id: upload.id },
    data: { atlasGuid: atlasResponse.guid }
  });
}
```

### 4. Solr Query Optimization

```bash
# Use filter queries (fq) for better caching
curl "http://localhost:8983/solr/vertex_index/select" \
  -d "q=mahasiswa" \
  -d "fq=dataLayer_s:SILVER" \
  -d "fq=createTime:[NOW-7DAYS TO NOW]" \
  -d "rows=20"

# Filter queries are cached separately and reused
# Main query: q=mahasiswa (cached)
# Filter: fq=dataLayer_s:SILVER (cached)
# Filter: fq=createTime:[...] (cached)
```

---

## ğŸ“Š Scalability Considerations

### HBase Region Splitting

```bash
# Pre-split regions for atlas_entity table
# Split by GUID prefix (first 2 chars)

create 'atlas_entity', 
  {NAME => 'cf_entity'}, 
  {NAME => 'cf_metadata'}, 
  {NAME => 'cf_classifications'},
  {SPLITS => [
    '00', '10', '20', '30', '40', '50', '60', '70', 
    '80', '90', 'a0', 'b0', 'c0', 'd0', 'e0', 'f0'
  ]}

# Result: 16 regions, each handling ~6.25% of data
# Enables parallel reads/writes across RegionServers
```

### Solr Sharding Strategy

```bash
# Create sharded collection (4 shards, 2 replicas each)
bin/solr create -c vertex_index \
  -shards 4 \
  -replicationFactor 2

# Documents distributed by hash(guid) % num_shards
# Shard 1: GUIDs 0-fffff... (25%)
# Shard 2: GUIDs 1-fffff... (25%)
# Shard 3: GUIDs 2-fffff... (25%)
# Shard 4: GUIDs 3-fffff... (25%)

# Each shard has 2 replicas for high availability
```

### Cluster Sizing Guidelines

| Component | Small (< 10K entities) | Medium (< 100K entities) | Large (> 100K entities) |
|-----------|------------------------|--------------------------|-------------------------|
| **HBase** | 3 RegionServers (4GB each) | 5 RegionServers (8GB each) | 10+ RegionServers (16GB each) |
| **Solr** | 2 nodes (4GB each) | 4 nodes (8GB each) | 8+ nodes (16GB each) |
| **Zookeeper** | 3 nodes (2GB each) | 3 nodes (4GB each) | 5 nodes (4GB each) |
| **Atlas** | 1 instance (8GB) | 2 instances (16GB) | 3+ instances (32GB) |

### Monitoring Metrics

```javascript
// backend/src/services/atlas-monitoring.service.js

async function getAtlasMetrics() {
  const metrics = await atlasService.getMetrics();
  
  return {
    // Entity counts
    entityCounts: {
      total: metrics.entity?.entityActive || 0,
      bronze: metrics.tag?.tagEntities?.Bronze || 0,
      silver: metrics.tag?.tagEntities?.Silver || 0,
      gold: metrics.tag?.tagEntities?.Gold || 0
    },
    
    // Performance
    performance: {
      avgEntityCreateTime: metrics.server?.timers?.entityCreate?.mean || 0,
      avgSearchTime: metrics.server?.timers?.search?.mean || 0
    },
    
    // Storage
    storage: {
      hbaseSize: metrics.system?.hbase?.storeFileSize || 0,
      solrIndexSize: metrics.system?.solr?.indexSize || 0
    },
    
    // Health
    health: {
      hbaseAvailable: metrics.system?.hbase?.available || false,
      solrAvailable: metrics.system?.solr?.available || false,
      zookeeperAvailable: metrics.system?.zookeeper?.available || false
    }
  };
}
```

---

## ğŸ† Patent Claims Summary

### Novel Innovations

#### 1. **Automated Bidirectional Metadata Synchronization**

**Claim**: A system for automatically synchronizing metadata between a relational database (PostgreSQL) and a distributed metadata repository (Apache Atlas) using unique identifiers (GUIDs), enabling:
- Zero-latency metadata lookup via PostgreSQL â†’ Atlas GUID mapping
- Consistent metadata state across transactional and analytical systems
- Fallback mechanisms when metadata services are unavailable

**Prior Art Differentiation**: Traditional systems require manual metadata registration or batch synchronization. Our system performs real-time, event-driven synchronization at upload time.

#### 2. **Multi-Layer Metadata Storage Architecture**

**Claim**: A hierarchical metadata storage system utilizing:
- **HBase**: Persistent storage for entity attributes and relationships
- **Solr**: Real-time searchable index with faceted navigation
- **Zookeeper**: Distributed coordination for consistency
- **PostgreSQL**: Transactional metadata with business context

**Innovation**: Each layer serves specific query patterns (point lookups, full-text search, transactions) while maintaining eventual consistency.

#### 3. **Automatic Lineage Graph Construction from ETL Processes**

**Claim**: A method for automatically generating data lineage graphs by:
1. Capturing source entity GUIDs before transformation
2. Collecting transformation metadata during processing
3. Creating process entities linking inputs to outputs
4. Storing lineage as directed acyclic graph (DAG) in Atlas

**Value**: Complete end-to-end lineage without manual annotation. Enables impact analysis, root cause diagnosis, and compliance tracking.

#### 4. **Classification Inheritance Across Data Layers**

**Claim**: Automatic propagation of sensitive data classifications (PII, Confidential) from source (Bronze) to derived (Silver) entities, ensuring:
- Compliance tags follow data through transformations
- No manual re-classification needed
- Audit trail of classification lineage

**Impact**: Prevents data governance gaps when sensitive data flows through pipelines.

#### 5. **Transformation Metadata Capture in Distributed Processing**

**Claim**: A system for capturing detailed transformation metrics from distributed Spark jobs:
- Records processed, duplicates removed, nulls handled
- Data quality scores before/after transformation
- Processing time and resource utilization
- Storage as searchable metadata in Atlas

**Novelty**: Transformation metadata stored as first-class entities, queryable alongside data entities.

#### 6. **Hybrid Search Architecture (SQL + Solr + Graph)**

**Claim**: Unified search interface combining:
- SQL queries against PostgreSQL (business metadata)
- Solr full-text search (technical metadata)
- Graph traversal queries (lineage relationships)

**Advantage**: Single API endpoint for diverse metadata queries, optimized routing to appropriate backend.

---

## ğŸ“ Summary - Part 2

### Key Components Delivered

1. **Spark-Atlas Integration**
   - PySpark Atlas client library
   - Silver entity registration from Spark jobs
   - ETL process entity creation for lineage
   - Classification inheritance logic

2. **Search & Discovery**
   - Solr indexing schema and queries
   - Faceted search by layer, type, owner
   - Full-text search across metadata
   - Date range and quality score filters

3. **Lineage Tracking**
   - Automatic lineage graph construction
   - HBase storage of relationships
   - Lineage query API for visualization
   - Multi-hop upstream/downstream traversal

4. **Performance Optimization**
   - Bulk entity creation
   - Redis caching for frequent queries
   - Async Atlas registration
   - Solr query optimization

5. **Scalability**
   - HBase region splitting strategy
   - Solr sharding configuration
   - Cluster sizing guidelines
   - Monitoring metrics

### Patent-Worthy Innovations

âœ… **Automated bidirectional metadata sync** (PostgreSQL â†” Atlas)  
âœ… **Multi-layer metadata storage** (HBase + Solr + Zookeeper)  
âœ… **Automatic lineage graph construction** from ETL processes  
âœ… **Classification inheritance** across data layers  
âœ… **Transformation metadata capture** in distributed Spark  
âœ… **Hybrid search architecture** (SQL + Solr + Graph)  

### Implementation Status

| Component | Status | Location |
|-----------|--------|----------|
| Atlas Client (Python) | âœ… Ready | `data-layer/spark-jobs/utils/atlas_client.py` |
| Spark Integration | âœ… Implemented | `bronze_to_silver_transformation.py` |
| Search API | âœ… Available | `backend/src/controllers/atlas.controller.js` |
| Lineage API | âœ… Available | `backend/src/controllers/atlas.controller.js` |
| Solr Schema | âœ… Configured | Atlas managed |
| HBase Tables | âœ… Auto-created | Atlas managed |

### Next Steps for Production

1. **Performance Testing**
   - Load test with 100K+ entities
   - Measure search latency at scale
   - Optimize HBase region splits

2. **Monitoring Setup**
   - Atlas metrics dashboard
   - HBase/Solr health checks
   - Alert rules for failures

3. **Documentation**
   - API documentation
   - Lineage visualization guide
   - Search query examples

4. **Patent Filing**
   - Prepare detailed claims
   - Create architecture diagrams
   - Document prior art analysis

---

**Document Version**: 1.0  
**Created**: November 28, 2025  
**Author**: Portal INSIGHTERA Team  
**Patent Classification**: Big Data Metadata Management System - Part 2
