# Metadata Management in Bronze to Silver ETL Pipeline - Part 1

**Patent Documentation: Big Data Metadata Processing & Storage**

---

## ğŸ“‹ Table of Contents

1. [Executive Summary](#executive-summary)
2. [Metadata Architecture Overview](#metadata-architecture-overview)
3. [Apache Atlas Technology Stack](#apache-atlas-technology-stack)
4. [Metadata Lifecycle in ETL Pipeline](#metadata-lifecycle-in-etl-pipeline)
5. [Metadata Capture & Registration](#metadata-capture--registration)
6. [Metadata Storage in Atlas](#metadata-storage-in-atlas)
7. [Metadata Enrichment Process](#metadata-enrichment-process)
8. [Lineage Tracking Implementation](#lineage-tracking-implementation)
9. [Search & Discovery (Solr)](#search--discovery-solr)
10. [Performance & Scalability](#performance--scalability)

---

## ğŸ¯ Executive Summary

### Innovation Overview

Portal INSIGHTERA mengimplementasikan **automated metadata management system** yang terintegrasi dengan **Apache Atlas** untuk tracking, governance, dan lineage dari data yang mengalir melalui Bronze to Silver ETL pipeline.

**Key Innovations**:
1. âœ… **Automatic Metadata Capture** - Metadata ter-register otomatis saat file upload
2. âœ… **Real-time Lineage Tracking** - Track transformasi Bronze â†’ Silver dengan detail lengkap
3. âœ… **Multi-layer Storage** - Atlas (HBase) + Solr + Zookeeper untuk high availability
4. âœ… **Bidirectional Integration** - Portal backend â†” Spark ETL â†” Atlas
5. âœ… **Rich Metadata Model** - Technical, business, operational metadata terintegrasi

### Technology Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  APACHE ATLAS - Metadata & Governance Platform                  â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  Component Stack:                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   SOLR     â”‚  â”‚   HBASE    â”‚  â”‚ ZOOKEEPER  â”‚               â”‚
â”‚  â”‚ (Search &  â”‚  â”‚ (Metadata  â”‚  â”‚(Coordinationâ”‚               â”‚
â”‚  â”‚  Indexing) â”‚  â”‚  Storage)  â”‚  â”‚  Service)  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                  â”‚
â”‚  Atlas REST API (Port 21000)                                    â”‚
â”‚  - Entity Management                                            â”‚
â”‚  - Type Definitions                                             â”‚
â”‚  - Lineage Tracking                                             â”‚
â”‚  - Search & Discovery                                           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                â”‚
           â”‚                                â”‚
           â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PORTAL BACKEND      â”‚         â”‚  SPARK ETL JOBS      â”‚
â”‚  (Node.js)           â”‚         â”‚  (PySpark)           â”‚
â”‚                      â”‚         â”‚                      â”‚
â”‚  - atlas.service.js  â”‚         â”‚  - Metadata writer   â”‚
â”‚  - Auto-registration â”‚         â”‚  - Lineage tracker   â”‚
â”‚  - GUID management   â”‚         â”‚  - Quality metrics   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Metadata Architecture Overview

### Metadata Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 1: FILE UPLOAD & INITIAL REGISTRATION                    â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  User uploads file â†’ Portal Backend                             â”‚
â”‚                                                                  â”‚
â”‚  1ï¸âƒ£ File saved to ADLS Bronze container                        â”‚
â”‚     Path: bronze/akademik/123/mahasiswa.csv                     â”‚
â”‚                                                                  â”‚
â”‚  2ï¸âƒ£ Create record in PostgreSQL database                       â”‚
â”‚     Table: uploads                                              â”‚
â”‚     Columns: id, fileName, filePath, uploadedBy, ...            â”‚
â”‚                                                                  â”‚
â”‚  3ï¸âƒ£ AUTOMATIC ATLAS REGISTRATION                               â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚ atlas.service.createADLSEntity()                â”‚        â”‚
â”‚     â”‚                                                 â”‚        â”‚
â”‚     â”‚ POST /api/atlas/v2/entity                       â”‚        â”‚
â”‚     â”‚ {                                               â”‚        â”‚
â”‚     â”‚   "entity": {                                   â”‚        â”‚
â”‚     â”‚     "typeName": "adls_gen2_resource",          â”‚        â”‚
â”‚     â”‚     "attributes": {                             â”‚        â”‚
â”‚     â”‚       "qualifiedName": "bronze://...",         â”‚        â”‚
â”‚     â”‚       "name": "mahasiswa.csv",                 â”‚        â”‚
â”‚     â”‚       "owner": "staff@univ.ac.id",             â”‚        â”‚
â”‚     â”‚       "storageAccount": "insighteradl",        â”‚        â”‚
â”‚     â”‚       "container": "bronze",                   â”‚        â”‚
â”‚     â”‚       "path": "akademik/123/mahasiswa.csv",    â”‚        â”‚
â”‚     â”‚       "fileType": "csv",                       â”‚        â”‚
â”‚     â”‚       "sizeBytes": 2048576,                    â”‚        â”‚
â”‚     â”‚       "recordCount": 1500,                     â”‚        â”‚
â”‚     â”‚       "dataLayer": "BRONZE",                   â”‚        â”‚
â”‚     â”‚       "uploadDate": 1732780800000              â”‚        â”‚
â”‚     â”‚     }                                           â”‚        â”‚
â”‚     â”‚   }                                             â”‚        â”‚
â”‚     â”‚ }                                               â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                  â”‚
â”‚  4ï¸âƒ£ Atlas returns GUID                                         â”‚
â”‚     Example: "a1b2c3d4-e5f6-7890-abcd-ef1234567890"            â”‚
â”‚                                                                  â”‚
â”‚  5ï¸âƒ£ Update PostgreSQL with Atlas GUID                          â”‚
â”‚     UPDATE uploads SET atlasGuid = 'a1b2c3d4...' WHERE id = 123â”‚
â”‚                                                                  â”‚
â”‚  6ï¸âƒ£ Add classification tag to entity                           â”‚
â”‚     atlas.service.addClassifications(guid, ['Bronze'])          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 2: STAGING ANALYSIS & METADATA ENRICHMENT                â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  Staging process (WOA + EDA) generates additional metadata:     â”‚
â”‚                                                                  â”‚
â”‚  - Data quality score: 95.5%                                    â”‚
â”‚  - Column statistics: min, max, mean, stddev, null_count        â”‚
â”‚  - Data profiling results: patterns, outliers                   â”‚
â”‚  - Schema inference: detected types, constraints                â”‚
â”‚                                                                  â”‚
â”‚  âš ï¸ At this stage, metadata NOT yet updated in Atlas           â”‚
â”‚  (Will be updated after Silver transformation)                  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 3: ETL PIPELINE TRIGGER & LINEAGE PREPARATION            â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  User clicks "Promote to Silver"                                â”‚
â”‚                                                                  â”‚
â”‚  1ï¸âƒ£ Portal Backend triggers Airflow DAG                        â”‚
â”‚     POST /api/v1/dags/bronze_to_silver_spark_pipeline/dagRuns   â”‚
â”‚                                                                  â”‚
â”‚  2ï¸âƒ£ DAG configuration includes Atlas metadata                  â”‚
â”‚     {                                                            â”‚
â”‚       "upload_id": 123,                                          â”‚
â”‚       "bronze_atlas_guid": "a1b2c3d4-...",  â† Atlas GUID       â”‚
â”‚       "bronze_path": "abfss://bronze@.../mahasiswa.csv",        â”‚
â”‚       "data_mart_code": "akademik",                             â”‚
â”‚       "quality_score": 95.5                                      â”‚
â”‚     }                                                            â”‚
â”‚                                                                  â”‚
â”‚  3ï¸âƒ£ Airflow DAG starts execution                               â”‚
â”‚     Task 1: prepare_spark_config                                â”‚
â”‚     - Fetch Bronze entity metadata from Atlas                   â”‚
â”‚     - Prepare Silver output paths                               â”‚
â”‚     - Pass Atlas GUID to Spark job                              â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 4: SPARK ETL EXECUTION & METADATA CREATION               â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  PySpark job: bronze_to_silver_transformation.py                â”‚
â”‚                                                                  â”‚
â”‚  1ï¸âƒ£ Read Bronze data (Extract phase)                           â”‚
â”‚     df = spark.read.csv(bronze_path)                            â”‚
â”‚     original_count = df.count()                                 â”‚
â”‚                                                                  â”‚
â”‚  2ï¸âƒ£ Apply transformations (Transform phase)                    â”‚
â”‚     - Standardize column names                                  â”‚
â”‚     - Remove duplicates                                         â”‚
â”‚     - Handle missing values                                     â”‚
â”‚     - Convert data types                                        â”‚
â”‚     - Add metadata columns                                      â”‚
â”‚     - Apply quality rules                                       â”‚
â”‚                                                                  â”‚
â”‚  3ï¸âƒ£ COLLECT PROCESSING METADATA                                â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚ transformation_metadata = {                     â”‚        â”‚
â”‚     â”‚   "original_count": 1500,                       â”‚        â”‚
â”‚     â”‚   "duplicates_removed": 10,                     â”‚        â”‚
â”‚     â”‚   "nulls_filled": 25,                           â”‚        â”‚
â”‚     â”‚   "type_conversions": 5,                        â”‚        â”‚
â”‚     â”‚   "final_count": 1490,                          â”‚        â”‚
â”‚     â”‚   "quality_score": 98.3,                        â”‚        â”‚
â”‚     â”‚   "processing_time_seconds": 45.2,              â”‚        â”‚
â”‚     â”‚   "columns_standardized": 15,                   â”‚        â”‚
â”‚     â”‚   "quality_rules_applied": 8,                   â”‚        â”‚
â”‚     â”‚   "quality_rules_passed": 1465,                 â”‚        â”‚
â”‚     â”‚   "quality_rules_failed": 25                    â”‚        â”‚
â”‚     â”‚ }                                                â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                  â”‚
â”‚  4ï¸âƒ£ Write to Silver layer (Load phase)                         â”‚
â”‚     silver_path = "silver/akademik/2025/11/28/upload_123/"      â”‚
â”‚     df.write.partitionBy(...).parquet(silver_path)              â”‚
â”‚                                                                  â”‚
â”‚  5ï¸âƒ£ REGISTER SILVER ENTITY IN ATLAS                            â”‚
â”‚     (Via Atlas REST API from Spark job)                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚ POST /api/atlas/v2/entity                       â”‚        â”‚
â”‚     â”‚ {                                               â”‚        â”‚
â”‚     â”‚   "entity": {                                   â”‚        â”‚
â”‚     â”‚     "typeName": "adls_gen2_resource",          â”‚        â”‚
â”‚     â”‚     "attributes": {                             â”‚        â”‚
â”‚     â”‚       "qualifiedName": "silver://...",         â”‚        â”‚
â”‚     â”‚       "name": "mahasiswa_silver",              â”‚        â”‚
â”‚     â”‚       "owner": "etl_system",                   â”‚        â”‚
â”‚     â”‚       "container": "silver",                   â”‚        â”‚
â”‚     â”‚       "path": "akademik/2025/11/28/...",       â”‚        â”‚
â”‚     â”‚       "fileType": "parquet",                   â”‚        â”‚
â”‚     â”‚       "recordCount": 1490,                     â”‚        â”‚
â”‚     â”‚       "dataLayer": "SILVER",                   â”‚        â”‚
â”‚     â”‚       "processingDate": 1732780800000,         â”‚        â”‚
â”‚     â”‚       "qualityScore": 98.3,                    â”‚        â”‚
â”‚     â”‚       "sourceUploadId": 123                    â”‚        â”‚
â”‚     â”‚     }                                           â”‚        â”‚
â”‚     â”‚   }                                             â”‚        â”‚
â”‚     â”‚ }                                               â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                  â”‚
â”‚  6ï¸âƒ£ CREATE ETL PROCESS ENTITY (Lineage)                        â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚ POST /api/atlas/v2/entity                       â”‚        â”‚
â”‚     â”‚ {                                               â”‚        â”‚
â”‚     â”‚   "entity": {                                   â”‚        â”‚
â”‚     â”‚     "typeName": "etl_process",                 â”‚        â”‚
â”‚     â”‚     "attributes": {                             â”‚        â”‚
â”‚     â”‚       "qualifiedName": "bronze_to_silver_123", â”‚        â”‚
â”‚     â”‚       "name": "Bronzeâ†’Silver ETL",             â”‚        â”‚
â”‚     â”‚       "inputs": [                               â”‚        â”‚
â”‚     â”‚         {"guid": "a1b2c3d4-..."}  â† Bronze     â”‚        â”‚
â”‚     â”‚       ],                                        â”‚        â”‚
â”‚     â”‚       "outputs": [                              â”‚        â”‚
â”‚     â”‚         {"guid": "f9e8d7c6-..."}  â† Silver     â”‚        â”‚
â”‚     â”‚       ],                                        â”‚        â”‚
â”‚     â”‚       "processType": "SPARK_ETL",              â”‚        â”‚
â”‚     â”‚       "transformations": {                      â”‚        â”‚
â”‚     â”‚         "duplicates_removed": 10,              â”‚        â”‚
â”‚     â”‚         "nulls_filled": 25,                    â”‚        â”‚
â”‚     â”‚         "type_conversions": 5                  â”‚        â”‚
â”‚     â”‚       },                                        â”‚        â”‚
â”‚     â”‚       "executionTime": 45.2,                   â”‚        â”‚
â”‚     â”‚       "executedBy": "spark_user",              â”‚        â”‚
â”‚     â”‚       "executionDate": 1732780800000           â”‚        â”‚
â”‚     â”‚     }                                           â”‚        â”‚
â”‚     â”‚   }                                             â”‚        â”‚
â”‚     â”‚ }                                               â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                  â”‚
â”‚  7ï¸âƒ£ Add Silver classification                                  â”‚
â”‚     atlas.addClassifications(silver_guid, ['Silver'])           â”‚
â”‚                                                                  â”‚
â”‚  8ï¸âƒ£ Return metadata to Airflow                                 â”‚
â”‚     {                                                            â”‚
â”‚       "status": "SUCCESS",                                       â”‚
â”‚       "silver_atlas_guid": "f9e8d7c6-...",                      â”‚
â”‚       "etl_process_guid": "b2c3d4e5-...",                       â”‚
â”‚       "transformation_metadata": {...}                           â”‚
â”‚     }                                                            â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 5: WEBHOOK CALLBACK & DATABASE UPDATE                    â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  Airflow calls Portal webhook:                                  â”‚
â”‚  POST /api/etl/webhook/complete                                 â”‚
â”‚                                                                  â”‚
â”‚  Payload:                                                        â”‚
â”‚  {                                                               â”‚
â”‚    "uploadId": 123,                                              â”‚
â”‚    "status": "SUCCESS",                                          â”‚
â”‚    "silverLayerPath": "silver/akademik/2025/11/28/...",         â”‚
â”‚    "silverAtlasGuid": "f9e8d7c6-...",                           â”‚
â”‚    "etlProcessGuid": "b2c3d4e5-...",                            â”‚
â”‚    "recordsProcessed": 1490,                                     â”‚
â”‚    "transformationsApplied": {...}                               â”‚
â”‚  }                                                               â”‚
â”‚                                                                  â”‚
â”‚  Portal Backend updates database:                               â”‚
â”‚  UPDATE uploads SET                                              â”‚
â”‚    silverLayerPath = 'silver/akademik/...',                     â”‚
â”‚    silverAtlasGuid = 'f9e8d7c6-...',                            â”‚
â”‚    uploadStatus = 'VALIDATED',                                  â”‚
â”‚    etlCompletedAt = NOW()                                        â”‚
â”‚  WHERE id = 123;                                                 â”‚
â”‚                                                                  â”‚
â”‚  INSERT INTO etl_jobs (                                          â”‚
â”‚    uploadId, dagRunId, jobStatus,                               â”‚
â”‚    bronzeAtlasGuid, silverAtlasGuid, etlProcessGuid,            â”‚
â”‚    transformationMetrics, ...                                    â”‚
â”‚  ) VALUES (...);                                                 â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Metadata Categories

| Category | Description | Storage Location | Examples |
|----------|-------------|------------------|----------|
| **Technical Metadata** | Schema, data types, file formats | Atlas (HBase) | Column names, types, nullability |
| **Operational Metadata** | Processing stats, timestamps | Atlas + PostgreSQL | Record counts, processing time |
| **Business Metadata** | Ownership, descriptions | Atlas | Owner name, data mart, purpose |
| **Quality Metadata** | Validation results, scores | Atlas + PostgreSQL | Quality score, rules passed/failed |
| **Lineage Metadata** | Data flow, transformations | Atlas (graph) | Bronze â†’ ETL Process â†’ Silver |

---

## ğŸ”§ Apache Atlas Technology Stack

### Component Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  APACHE ATLAS ARCHITECTURE                                      â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ATLAS REST API (Port 21000)                             â”‚  â”‚
â”‚  â”‚  - Entity CRUD operations                                â”‚  â”‚
â”‚  â”‚  - Type management                                       â”‚  â”‚
â”‚  â”‚  - Search & Discovery                                    â”‚  â”‚
â”‚  â”‚  - Lineage queries                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ATLAS CORE                                              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ Type System  â”‚  â”‚Graph Engine  â”‚  â”‚ Notification â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ - EntityDefs â”‚  â”‚ - Vertices    â”‚  â”‚ - Kafka/Hook â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ - ClassDefs  â”‚  â”‚ - Edges       â”‚  â”‚ - Listeners  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ - RelDefs    â”‚  â”‚ - Properties  â”‚  â”‚              â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                â”‚                      â”‚                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  APACHE HBASE         â”‚  â”‚  APACHE SOLR      â”‚             â”‚
â”‚  â”‚  (Metadata Storage)   â”‚  â”‚  (Search Index)   â”‚             â”‚
â”‚  â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚  â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚             â”‚
â”‚  â”‚                       â”‚  â”‚                   â”‚             â”‚
â”‚  â”‚  Tables:              â”‚  â”‚  Collections:     â”‚             â”‚
â”‚  â”‚  - atlas_entity       â”‚  â”‚  - vertex_index   â”‚             â”‚
â”‚  â”‚  - atlas_relationship â”‚  â”‚  - edge_index     â”‚             â”‚
â”‚  â”‚  - atlas_audit        â”‚  â”‚  - fulltext_index â”‚             â”‚
â”‚  â”‚                       â”‚  â”‚                   â”‚             â”‚
â”‚  â”‚  Stores:              â”‚  â”‚  Features:        â”‚             â”‚
â”‚  â”‚  - Entity attributes  â”‚  â”‚  - Full-text      â”‚             â”‚
â”‚  â”‚  - Relationships      â”‚  â”‚  - Faceted search â”‚             â”‚
â”‚  â”‚  - Audit logs         â”‚  â”‚  - Wildcards      â”‚             â”‚
â”‚  â”‚  - Classifications    â”‚  â”‚  - Fuzzy matching â”‚             â”‚
â”‚  â”‚                       â”‚  â”‚                   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚              â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  APACHE ZOOKEEPER                                        â”‚  â”‚
â”‚  â”‚  (Coordination Service)                                  â”‚  â”‚
â”‚  â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                                   â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  Manages:                                                â”‚  â”‚
â”‚  â”‚  - HBase region servers                                  â”‚  â”‚
â”‚  â”‚  - Solr cloud nodes                                      â”‚  â”‚
â”‚  â”‚  - Leader election                                       â”‚  â”‚
â”‚  â”‚  - Configuration management                              â”‚  â”‚
â”‚  â”‚  - Service discovery                                     â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Roles

#### 1. **Apache HBase** - Primary Metadata Storage

**Purpose**: Store entity metadata, relationships, and audit logs

**Schema Design**:
```
Table: atlas_entity
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Row Key: <entity_guid>
Column Families:
  - cf_entity: Entity attributes
  - cf_metadata: System metadata (created, updated, version)
  - cf_classifications: Applied tags/classifications

Example Row:
Row: a1b2c3d4-e5f6-7890-abcd-ef1234567890
  cf_entity:
    typeName: "adls_gen2_resource"
    qualifiedName: "bronze://akademik/123/mahasiswa.csv"
    name: "mahasiswa.csv"
    owner: "staff@univ.ac.id"
    storageAccount: "insighteradl"
    container: "bronze"
    path: "akademik/123/mahasiswa.csv"
    fileType: "csv"
    sizeBytes: 2048576
    recordCount: 1500
    dataLayer: "BRONZE"
  cf_metadata:
    createdBy: "atlas_admin"
    createTime: 1732780800000
    modifiedBy: "atlas_admin"
    modifyTime: 1732780800000
    version: 1
  cf_classifications:
    Bronze: {"typeName": "Bronze"}
```

**Why HBase?**
- âœ… **Horizontal scalability** - Handle millions of entities
- âœ… **Fast random access** - Get entity by GUID in milliseconds
- âœ… **Column-oriented** - Efficient storage for sparse attributes
- âœ… **Versioning** - Track metadata changes over time
- âœ… **Strong consistency** - ACID guarantees via Zookeeper

#### 2. **Apache Solr** - Search & Indexing

**Purpose**: Enable fast full-text search and faceted queries

**Collections**:
```
Collection: vertex_index
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Documents indexed from HBase entities

Example Document:
{
  "guid": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "typeName": "adls_gen2_resource",
  "qualifiedName": "bronze://akademik/123/mahasiswa.csv",
  "name": "mahasiswa.csv",
  "owner": "staff@univ.ac.id",
  "dataLayer": "BRONZE",
  "container": "bronze",
  "classifications": ["Bronze"],
  "createTime": "2025-11-28T10:00:00Z",
  "modifyTime": "2025-11-28T10:00:00Z",
  
  // Full-text indexed fields
  "text": "mahasiswa.csv staff@univ.ac.id bronze akademik",
  
  // Facets for filtering
  "dataLayer_s": "BRONZE",
  "typeName_s": "adls_gen2_resource",
  "owner_s": "staff@univ.ac.id"
}
```

**Search Capabilities**:
```
# Find all Bronze layer files
q=dataLayer_s:BRONZE

# Find files by owner
q=owner_s:"staff@univ.ac.id"

# Full-text search
q=text:mahasiswa

# Faceted search (count by type)
q=*:*&facet=true&facet.field=typeName_s

# Wildcard search
q=name:mahasiswa*

# Date range
q=createTime:[2025-11-01T00:00:00Z TO 2025-11-30T23:59:59Z]
```

**Why Solr?**
- âœ… **Fast full-text search** - Inverted index for text fields
- âœ… **Faceted navigation** - Group by type, owner, layer
- âœ… **Wildcard/fuzzy** - Flexible search patterns
- âœ… **Real-time indexing** - Changes appear in seconds
- âœ… **Distributed** - SolrCloud for high availability

#### 3. **Apache Zookeeper** - Coordination Service

**Purpose**: Manage distributed components and ensure consistency

**Responsibilities**:

1. **HBase Coordination**:
   - Track active RegionServers
   - Manage region assignments
   - Handle failover/recovery

2. **Solr Cloud Coordination**:
   - Leader election for shards
   - Cluster state management
   - Configuration distribution

3. **Atlas High Availability**:
   - Active/passive Atlas instances
   - Automatic failover
   - Session management

**Why Zookeeper?**
- âœ… **Leader election** - Ensure single writer
- âœ… **Configuration sync** - Consistent cluster state
- âœ… **Service discovery** - Dynamic node registration
- âœ… **Locks & barriers** - Distributed coordination
- âœ… **Fault tolerance** - Quorum-based consensus

---

## ğŸ”„ Metadata Lifecycle in ETL Pipeline

### Phase-by-Phase Metadata Processing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: BRONZE LAYER - Initial Metadata Capture               â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  Trigger: File upload to Portal                                 â”‚
â”‚  Location: Portal Backend (Node.js)                             â”‚
â”‚  Service: atlas.service.js                                      â”‚
â”‚                                                                  â”‚
â”‚  Metadata Captured:                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ TECHNICAL METADATA                                 â”‚         â”‚
â”‚  â”‚ - qualifiedName: Unique identifier                â”‚         â”‚
â”‚  â”‚ - name: Human-readable file name                  â”‚         â”‚
â”‚  â”‚ - fileType: csv | parquet | json | xlsx           â”‚         â”‚
â”‚  â”‚ - sizeBytes: File size in bytes                   â”‚         â”‚
â”‚  â”‚ - recordCount: Estimated row count                â”‚         â”‚
â”‚  â”‚ - storageAccount: ADLS account name               â”‚         â”‚
â”‚  â”‚ - container: bronze                               â”‚         â”‚
â”‚  â”‚ - path: Full path in container                    â”‚         â”‚
â”‚  â”‚                                                    â”‚         â”‚
â”‚  â”‚ BUSINESS METADATA                                 â”‚         â”‚
â”‚  â”‚ - owner: User who uploaded                        â”‚         â”‚
â”‚  â”‚ - description: User-provided description          â”‚         â”‚
â”‚  â”‚ - dataLayer: BRONZE                               â”‚         â”‚
â”‚  â”‚ - dataMart: akademik | keuangan | ...            â”‚         â”‚
â”‚  â”‚                                                    â”‚         â”‚
â”‚  â”‚ OPERATIONAL METADATA                              â”‚         â”‚
â”‚  â”‚ - uploadDate: Timestamp (epoch millis)            â”‚         â”‚
â”‚  â”‚ - uploadedBy: Email of uploader                   â”‚         â”‚
â”‚  â”‚ - sourceUploadId: Link to PostgreSQL uploads      â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                  â”‚
â”‚  Storage:                                                        â”‚
â”‚  - HBase: Full entity attributes                                â”‚
â”‚  - Solr: Indexed for search                                     â”‚
â”‚  - PostgreSQL: uploads.atlasGuid = <guid>                       â”‚
â”‚                                                                  â”‚
â”‚  Classifications Applied:                                        â”‚
â”‚  - "Bronze" (data layer tag)                                    â”‚
â”‚  - Optional: "PII", "Sensitive" (based on content)              â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: STAGING ANALYSIS - Metadata Enrichment                â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  Trigger: Staging analysis (WOA + EDA)                          â”‚
â”‚  Location: Portal Backend                                       â”‚
â”‚  Storage: PostgreSQL only (not yet in Atlas)                    â”‚
â”‚                                                                  â”‚
â”‚  Additional Metadata Generated:                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ DATA QUALITY METRICS                               â”‚         â”‚
â”‚  â”‚ - stagingScore: Overall quality (0-100)           â”‚         â”‚
â”‚  â”‚ - completenessScore: % non-null values            â”‚         â”‚
â”‚  â”‚ - uniquenessScore: % unique values                â”‚         â”‚
â”‚  â”‚ - validityScore: % valid formats                  â”‚         â”‚
â”‚  â”‚ - consistencyScore: % consistent values           â”‚         â”‚
â”‚  â”‚                                                    â”‚         â”‚
â”‚  â”‚ COLUMN PROFILING                                  â”‚         â”‚
â”‚  â”‚ For each column:                                  â”‚         â”‚
â”‚  â”‚ - dataType: Inferred type                         â”‚         â”‚
â”‚  â”‚ - nullCount: # of nulls                           â”‚         â”‚
â”‚  â”‚ - distinctCount: # of unique values               â”‚         â”‚
â”‚  â”‚ - min / max: Range (numeric/date)                â”‚         â”‚
â”‚  â”‚ - mean / stddev: Statistics (numeric)            â”‚         â”‚
â”‚  â”‚ - topValues: Most frequent values (string)        â”‚         â”‚
â”‚  â”‚ - patterns: Detected patterns (regex)             â”‚         â”‚
â”‚  â”‚ - outliers: Anomalous values                      â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                  â”‚
â”‚  âš ï¸ Note: This metadata stored in PostgreSQL staging_results   â”‚
â”‚  table. Will be sent to Atlas during ETL execution.             â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3: ETL EXECUTION - Transformation Metadata               â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  Trigger: Airflow DAG execution                                 â”‚
â”‚  Location: Spark job (bronze_to_silver_transformation.py)       â”‚
â”‚  Service: Atlas REST API calls from PySpark                     â”‚
â”‚                                                                  â”‚
â”‚  Step 1: Read Bronze metadata                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ # In Spark job initialization                      â”‚         â”‚
â”‚  â”‚ bronze_guid = config['bronze_atlas_guid']          â”‚         â”‚
â”‚  â”‚                                                     â”‚         â”‚
â”‚  â”‚ # Fetch Bronze entity from Atlas                   â”‚         â”‚
â”‚  â”‚ bronze_entity = atlas_client.get_entity(bronze_guidâ”‚         â”‚
â”‚  â”‚                                                     â”‚         â”‚
â”‚  â”‚ # Extract metadata                                 â”‚         â”‚
â”‚  â”‚ bronze_qualified_name = bronze_entity['qualifiedName'â”‚        â”‚
â”‚  â”‚ bronze_owner = bronze_entity['owner']              â”‚         â”‚
â”‚  â”‚ bronze_path = bronze_entity['path']                â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                  â”‚
â”‚  Step 2: Collect transformation metrics                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ transformation_metadata = {                        â”‚         â”‚
â”‚  â”‚   "original_count": 1500,                          â”‚         â”‚
â”‚  â”‚   "duplicates_removed": 10,                        â”‚         â”‚
â”‚  â”‚   "nulls_filled": 25,                              â”‚         â”‚
â”‚  â”‚   "type_conversions": 5,                           â”‚         â”‚
â”‚  â”‚   "final_count": 1490,                             â”‚         â”‚
â”‚  â”‚   "quality_score": 98.3,                           â”‚         â”‚
â”‚  â”‚   "processing_time_seconds": 45.2,                 â”‚         â”‚
â”‚  â”‚   "columns_before": 18,                            â”‚         â”‚
â”‚  â”‚   "columns_after": 24, # Added metadata cols       â”‚         â”‚
â”‚  â”‚   "transformations_applied": [                     â”‚         â”‚
â”‚  â”‚     "standardize_column_names",                    â”‚         â”‚
â”‚  â”‚     "remove_duplicates",                           â”‚         â”‚
â”‚  â”‚     "handle_missing_values",                       â”‚         â”‚
â”‚  â”‚     "convert_data_types",                          â”‚         â”‚
â”‚  â”‚     "add_metadata_columns",                        â”‚         â”‚
â”‚  â”‚     "apply_quality_rules"                          â”‚         â”‚
â”‚  â”‚   ]                                                 â”‚         â”‚
â”‚  â”‚ }                                                   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                  â”‚
â”‚  Step 3: Register Silver entity                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ silver_entity = {                                  â”‚         â”‚
â”‚  â”‚   "typeName": "adls_gen2_resource",               â”‚         â”‚
â”‚  â”‚   "attributes": {                                  â”‚         â”‚
â”‚  â”‚     "qualifiedName": "silver://akademik/...",     â”‚         â”‚
â”‚  â”‚     "name": "mahasiswa_silver",                   â”‚         â”‚
â”‚  â”‚     "owner": bronze_owner, # Inherit from Bronze  â”‚         â”‚
â”‚  â”‚     "container": "silver",                        â”‚         â”‚
â”‚  â”‚     "path": "akademik/2025/11/28/upload_123/",    â”‚         â”‚
â”‚  â”‚     "fileType": "parquet",                        â”‚         â”‚
â”‚  â”‚     "recordCount": 1490,                          â”‚         â”‚
â”‚  â”‚     "dataLayer": "SILVER",                        â”‚         â”‚
â”‚  â”‚     "processingDate": current_timestamp(),        â”‚         â”‚
â”‚  â”‚     "qualityScore": 98.3,                         â”‚         â”‚
â”‚  â”‚     "sourceUploadId": 123,                        â”‚         â”‚
â”‚  â”‚     "transformationMetrics": json.dumps(          â”‚         â”‚
â”‚  â”‚       transformation_metadata                     â”‚         â”‚
â”‚  â”‚     )                                              â”‚         â”‚
â”‚  â”‚   }                                                â”‚         â”‚
â”‚  â”‚ }                                                  â”‚         â”‚
â”‚  â”‚                                                    â”‚         â”‚
â”‚  â”‚ # POST to Atlas                                    â”‚         â”‚
â”‚  â”‚ response = atlas_client.create_entity(silver_entityâ”‚         â”‚
â”‚  â”‚ silver_guid = response['guidAssignments'][...]     â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                  â”‚
â”‚  Step 4: Create ETL Process entity (Lineage)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ etl_process = {                                    â”‚         â”‚
â”‚  â”‚   "typeName": "etl_process",                      â”‚         â”‚
â”‚  â”‚   "attributes": {                                  â”‚         â”‚
â”‚  â”‚     "qualifiedName": "bronze_to_silver_upload_123"â”‚         â”‚
â”‚  â”‚     "name": "Bronzeâ†’Silver: mahasiswa.csv",       â”‚         â”‚
â”‚  â”‚     "processType": "SPARK_ETL",                   â”‚         â”‚
â”‚  â”‚     "inputs": [                                    â”‚         â”‚
â”‚  â”‚       {"guid": bronze_guid}                       â”‚         â”‚
â”‚  â”‚     ],                                             â”‚         â”‚
â”‚  â”‚     "outputs": [                                   â”‚         â”‚
â”‚  â”‚       {"guid": silver_guid}                       â”‚         â”‚
â”‚  â”‚     ],                                             â”‚         â”‚
â”‚  â”‚     "transformations": transformation_metadata,    â”‚         â”‚
â”‚  â”‚     "executionTime": 45.2,                        â”‚         â”‚
â”‚  â”‚     "executedBy": "spark_etl_user",               â”‚         â”‚
â”‚  â”‚     "executionDate": current_timestamp(),         â”‚         â”‚
â”‚  â”‚     "dagId": "bronze_to_silver_spark_pipeline",   â”‚         â”‚
â”‚  â”‚     "dagRunId": "manual__2025-11-28T10:00:00",    â”‚         â”‚
â”‚  â”‚     "sparkAppId": "app-20251128100000-0001"       â”‚         â”‚
â”‚  â”‚   }                                                â”‚         â”‚
â”‚  â”‚ }                                                  â”‚         â”‚
â”‚  â”‚                                                    â”‚         â”‚
â”‚  â”‚ # POST to Atlas                                    â”‚         â”‚
â”‚  â”‚ response = atlas_client.create_entity(etl_process) â”‚         â”‚
â”‚  â”‚ etl_process_guid = response['guidAssignments'][...â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                  â”‚
â”‚  Step 5: Add classifications                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ # Add Silver classification                        â”‚         â”‚
â”‚  â”‚ atlas_client.add_classifications(                  â”‚         â”‚
â”‚  â”‚   silver_guid,                                     â”‚         â”‚
â”‚  â”‚   ['Silver']                                       â”‚         â”‚
â”‚  â”‚ )                                                  â”‚         â”‚
â”‚  â”‚                                                    â”‚         â”‚
â”‚  â”‚ # Copy classifications from Bronze                â”‚         â”‚
â”‚  â”‚ if bronze_entity.has('PII'):                       â”‚         â”‚
â”‚  â”‚   atlas_client.add_classifications(               â”‚         â”‚
â”‚  â”‚     silver_guid,                                   â”‚         â”‚
â”‚  â”‚     ['PII']                                        â”‚         â”‚
â”‚  â”‚   )                                                â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 4: POST-ETL - Database Synchronization                   â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                                  â”‚
â”‚  Trigger: Airflow webhook callback                              â”‚
â”‚  Location: Portal Backend                                       â”‚
â”‚  Service: ETL webhook controller                                â”‚
â”‚                                                                  â”‚
â”‚  Actions:                                                        â”‚
â”‚  1. Update uploads table                                        â”‚
â”‚     UPDATE uploads SET                                           â”‚
â”‚       silverLayerPath = 'silver/akademik/...',                  â”‚
â”‚       silverAtlasGuid = 'f9e8d7c6-...',                         â”‚
â”‚       uploadStatus = 'VALIDATED',                               â”‚
â”‚       recordCount = 1490,                                        â”‚
â”‚       etlCompletedAt = NOW()                                     â”‚
â”‚     WHERE id = 123;                                              â”‚
â”‚                                                                  â”‚
â”‚  2. Create etl_jobs record                                      â”‚
â”‚     INSERT INTO etl_jobs (                                       â”‚
â”‚       uploadId, jobName, jobStatus,                             â”‚
â”‚       dagId, dagRunId,                                           â”‚
â”‚       bronzeAtlasGuid, silverAtlasGuid, etlProcessGuid,         â”‚
â”‚       sourceLayer, targetLayer,                                  â”‚
â”‚       recordsProcessed, transformationMetrics,                   â”‚
â”‚       startedAt, completedAt                                     â”‚
â”‚     ) VALUES (                                                   â”‚
â”‚       123, 'Bronzeâ†’Silver: mahasiswa.csv', 'SUCCESS',           â”‚
â”‚       'bronze_to_silver_spark_pipeline', 'manual__...',         â”‚
â”‚       'a1b2c3d4-...', 'f9e8d7c6-...', 'b2c3d4e5-...',          â”‚
â”‚       'BRONZE', 'SILVER',                                        â”‚
â”‚       1490, '{"duplicates_removed": 10, ...}',                  â”‚
â”‚       '2025-11-28 10:00:00', '2025-11-28 10:01:30'             â”‚
â”‚     );                                                           â”‚
â”‚                                                                  â”‚
â”‚  Result:                                                         â”‚
â”‚  - PostgreSQL: Updated with Atlas GUIDs and ETL metrics         â”‚
â”‚  - Atlas HBase: Contains full lineage graph                     â”‚
â”‚  - Atlas Solr: Bronze + Silver entities indexed for search      â”‚
â”‚  - Users can now:                                               â”‚
â”‚    * Search for entities in BrowsePage                          â”‚
â”‚    * View lineage in EntityDetailModal                          â”‚
â”‚    * Query transformations in DataLineagePage                   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Metadata Capture & Registration

### Backend Implementation (Node.js)

**File**: `backend/src/controllers/file.controller.js`

```javascript
// After file upload to ADLS Bronze
async function uploadFileToBronze(req, res) {
  const { file, dataMartId, description } = req.body;
  
  try {
    // 1. Upload to ADLS
    const adlsResult = await adlsService.uploadFile(
      file,
      `bronze/${dataMart.code}/${upload.id}/${file.originalname}`
    );
    
    // 2. Create upload record in PostgreSQL
    const upload = await prisma.upload.create({
      data: {
        originalFileName: file.originalname,
        filePath: adlsResult.blobName,
        fileSize: file.size,
        fileType: getFileType(file.originalname),
        uploadedBy: req.user.email,
        dataMartId: dataMartId,
        uploadStatus: 'UPLOADED',
        // atlasGuid will be set after Atlas registration
      }
    });
    
    // 3. AUTO-REGISTER IN APACHE ATLAS
    try {
      console.log('[Upload] Registering entity in Apache Atlas...');
      
      const atlasResponse = await atlasService.createADLSEntity({
        fileName: file.originalname,
        containerName: 'bronze',
        blobPath: adlsResult.blobName,
        ownerName: req.user.name || req.user.email,
        uploadedBy: req.user.email,
        description: description || `File uploaded to ${dataMart.name}`,
        fileSize: file.size,
        fileType: getFileType(file.originalname),
        uploadedAt: new Date().toISOString(),
        dataLayer: 'BRONZE',
        dataMart: dataMart.name,
        recordCount: estimateRecordCount(file), // Optional
      });
      
      // Extract GUID from Atlas response
      const atlasGuid = atlasResponse.guidAssignments?.[
        Object.keys(atlasResponse.guidAssignments)[0]
      ];
      
      if (atlasGuid) {
        // 4. Add Bronze classification
        await atlasService.addClassifications(atlasGuid, ['Bronze']);
        
        // 5. Update upload with Atlas GUID
        await prisma.upload.update({
          where: { id: upload.id },
          data: { atlasGuid },
        });
        
        console.log(`[Upload] Entity registered with GUID: ${atlasGuid}`);
      }
      
    } catch (atlasError) {
      // Don't fail upload if Atlas registration fails
      console.error('[Upload] Atlas registration failed:', atlasError.message);
    }
    
    res.json({
      success: true,
      uploadId: upload.id,
      atlasGuid: upload.atlasGuid,
    });
    
  } catch (error) {
    console.error('[Upload] Failed:', error);
    res.status(500).json({ error: error.message });
  }
}
```

### Atlas Service Implementation

**File**: `backend/src/services/atlas.service.js`

```javascript
/**
 * Create ADLS Gen2 resource entity in Atlas
 */
async function createADLSEntity(fileData) {
  // Build qualifiedName (unique identifier)
  const qualifiedName = `adls://${fileData.containerName}/${fileData.blobPath}@insighteralake`;
  
  const attributes = {
    qualifiedName,
    name: fileData.fileName,
    storageAccount: 'insighteralake',
    container: fileData.containerName,
    path: fileData.blobPath,
    fileType: fileData.fileType,
    sizeBytes: fileData.fileSize,
    recordCount: fileData.recordCount || 0,
    uploadedBy: fileData.uploadedBy || fileData.ownerName,
    uploadDate: fileData.uploadedAt ? 
      new Date(fileData.uploadedAt).getTime() : Date.now(),
    dataLayer: fileData.dataLayer || 'BRONZE',
    owner: fileData.ownerName,
    description: fileData.description || '',
  };
  
  // Create entity via Atlas REST API
  return await createEntity('adls_gen2_resource', attributes);
}

/**
 * Generic create entity function
 */
async function createEntity(typeName, attributes) {
  const entity = {
    entity: {
      typeName,
      attributes,
      status: 'ACTIVE',
    },
  };
  
  console.log(`Creating Atlas entity: ${typeName}`);
  console.log('Attributes:', JSON.stringify(attributes, null, 2));
  
  const response = await atlasClient.post('/api/atlas/v2/entity', entity);
  
  console.log(`Entity created: ${typeName}`);
  console.log('GUIDs:', response.data.guidAssignments);
  
  return response.data;
}

/**
 * Add classifications (tags) to entity
 */
async function addClassifications(guid, classifications) {
  const classificationsPayload = classifications.map((name) => ({
    typeName: name,
  }));
  
  await atlasClient.post(
    `/api/atlas/v2/entity/guid/${guid}/classifications`,
    classificationsPayload
  );
  
  console.log(`Added classifications to ${guid}:`, classifications);
}
```

---

**[Continue to Part 2 for Silver Layer Metadata, Lineage Implementation, and Search/Discovery...]**

---

## ğŸ“Œ Summary - Part 1

### Key Innovations Documented

1. **Automated Metadata Capture**
   - Zero manual intervention
   - Metadata captured at upload time
   - Atlas GUID stored in PostgreSQL for bidirectional linking

2. **Technology Integration**
   - Apache Atlas REST API
   - HBase for metadata storage
   - Solr for search indexing
   - Zookeeper for coordination

3. **Metadata Categories**
   - Technical: Schema, types, formats
   - Business: Ownership, descriptions
   - Operational: Processing stats, timestamps
   - Quality: Validation results, scores

4. **Patent-worthy Components**
   - Automatic Bronze entity registration
   - GUID-based linking between PostgreSQL and Atlas
   - Classification inheritance (Bronze â†’ Silver)
   - Transformation metadata capture in Spark

### Next Steps

**Part 2 will cover:**
- Silver entity metadata registration from Spark
- ETL Process entity creation (lineage)
- Search & discovery with Solr
- Metadata querying and visualization
- Performance optimization techniques

---

**Document Version**: 1.0  
**Created**: November 28, 2025  
**Author**: Portal INSIGHTERA Team  
**Patent Classification**: Big Data Metadata Management System
